<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: transcription, | __del__( self )]]></title>
  <link href="http://Zulko.github.io/blog/categories/transcription/atom.xml" rel="self"/>
  <link href="http://Zulko.github.io/"/>
  <updated>2014-06-09T14:19:10+02:00</updated>
  <id>http://Zulko.github.io/</id>
  <author>
    <name><![CDATA[Zulko]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Transcribing Piano Rolls, the Pythonic Way]]></title>
    <link href="http://Zulko.github.io/blog/2014/02/12/transcribing-piano-rolls/"/>
    <updated>2014-02-12T00:25:00+01:00</updated>
    <id>http://Zulko.github.io/blog/2014/02/12/transcribing-piano-rolls</id>
    <content type="html"><![CDATA[<p><em>In this post I use Fourier transforms to revive a forgotten Gershwin piano piece.</em></p>

<!-- more -->

<p>Piano rolls are these rolls of perforated paper that you feed to the saloon’s mechanical piano. They have been very popular until the 1950s, and the piano roll repertory counts thousands of arrangements (some by greatest names of jazz) which have never been published in any other form.</p>

<p>Here is <em>Limehouse Nights</em> (~1918), by a 20-years-old George Gershwin:</p>

<p>{% youtube VjkS-XHScXU %}</p>

<p>It is cool, it is public domain music, and I want to play it. But like for so many other rolls, there is no published sheet music.</p>

<p>Fortunately, someone took a video of the same performance with a focus on the roll:</p>

<p>{% youtube wMsEbYCh7yY %}</p>

<p>In this post I show how to turn that video into playable sheet music with the help of a few lines of Python. <a href="#final_result">At the end</a> I provide the sheet music, a human rendition, and a Python package that implements the method (and can also be used to transcribe from MIDI files).</p>

<h2 id="downloading-the-video">Downloading the video</h2>

<p>You can download the video from Youtube using <a href="">youtube-dl</a> in a terminal:
{% codeblock  %}
youtube-dl wMsEbYCh7yY -o limehouse_nights.mp4
{% endcodeblock %}</p>

<h2 id="step-1-segmentation-of-the-roll">Step 1: Segmentation of the roll</h2>

<p>In each frame of the video we will focus on a well-located line of pixels:
{% img center /images/rolls_transcription/watched_line.jpeg %}</p>

<p>By extracting this line from each video frame and stacking the obtained 
lines on one another we can reconstitute an approximate <em>scan</em> of the piano roll:</p>

<p>{% codeblock lang:python %}</p>

<h1 id="required-python-modules">Required Python modules</h1>
<p>from moviepy.editor import VideoFileClip # for video processing
from pylab import * # for mathematics/plotting</p>

<h1 id="load-the-video-keep-the-clip-between-t2s-and-t-30s">load the video, keep the clip between t=2s and t= 30s</h1>
<p>video = VideoFileClip(‘./limehouse_nights.mp4’).subclip(2,30)</p>

<h1 id="extract-the-focus-lines-in-the-different-frames-stack-them">extract the focus lines in the different frames, stack them.</h1>
<p>roll_picture = vstack([frame[[156],58:478]
                       for frame in video.iter_frames()])</p>

<p>imshow( roll_picture ) # display the obtained picture
{% endcodeblock %}</p>

<p>{% img center /images/rolls_transcription/roll_RGB.jpeg %}</p>

<p>We can see that the holes are placed along columns. Each of these 
columns corresponds to one key of the piano. A possible way to find the 
x-coordinates of these columns in the picture is to look at the minimal 
luminosity of each column of pixels:</p>

<p>{% codeblock lang:python %}
roll_greyscale = roll_picture.mean(axis=2) # RGB to grey
luminosity_per_column = roll_greyscale.min(axis=0)</p>

<p>plot( luminosity_per_column)
xlabel(‘column of pixels (x-index)’)
ylabel(‘minimal luminosity’)
{% endcodeblock %}</p>

<p>{% img center /images/rolls_transcription/min_lum.jpeg %}</p>

<p>Holes are low-luminosity zones in the picture, therefore the x-coordinates with lower luminosity in the curve above indicate hole-columns. They are not equally spaced because some piano keys are not used in this piece, but there is clearly a dominant period, which we will find by looking at the
frequency spectrum of the curve.</p>

<p>We compute that spectrum using a continuous Fourier transform. The peaks in the spectrum below
mean that a periodic pattern is present in the curve:</p>

<p>{% codeblock lang:python %}
n_lines, n_columns = roll_greyscale.shape
tt = arange(n_columns) # 0,1,2,3,4… n_columns
lum0 = luminosity_per_column - luminosity_per_column.mean()</p>

<p>def fourier_transform(signal, period, tt):
    “”” See http://en.wikipedia.org/wiki/Fourier_transform
    I could also have used Numpy’s fft.
    “””
    f = lambda func : (signal<em>func(2</em>pi<em>tt/period)).sum()
    return f(cos)+ 1j</em>f(sin)</p>

<p>widths = arange(.1,20,.01)
transform = array([ fourier_transform(lum0,w,tt)
                    for w in widths])</p>

<p>plot(widths, abs(transform))
xlabel(“Period (in number of pixels)”)
ylabel(“Spectrum value”)
{% endcodeblock %}</p>

<p>{% img center /images/rolls_transcription/lum_spectrum.jpeg %}</p>

<p>The higher peak of the spectrum indicates a period of x=5.46 pixels, and this is indeed the distance in pixels between two hole-columns. This, plus the <em>phase</em> of the spectrum in this point, gives us the coordinates of the centers of the hole-columns (vertical lines below).</p>

<p>{% codeblock lang:python %}
# The maximum the transform indicates the holes’ width
optimal_i = argmax(abs(transform))
hole_width = widths[optimal_i]
offset = angle(transform[optimal_i]) +hole_width/2 # to be revised.</p>

<p>keys_positions = arange(offset, n_columns, hole_width)
keys_positions = np.round(keys_positions).astype(int)</p>

<p>plot(luminosity_per_column)
for h in keys_positions:
    axvline(h, c=’k’, alpha=0.5)
xlabel(‘column of pixels’)
ylabel(‘minimal luminosity’)
{% endcodeblock %}</p>

<p>{% img center /images/rolls_transcription/lum_plus_keycolumns.jpeg %}</p>

<p>We can now reduce our image the piano roll to keep only one pixel per hole column. In the resulting picture, one column gives the time profile of one key in the piano: when it is pressed, and when it is released.</p>

<p>{% codeblock lang:python %}
keys_greyscale = roll_greyscale[:, keys_positions]</p>

<p>imshow(keys_greyscale[0:150])
xlabel(‘piano-key column’)
ylabel(‘video frame number’)
{% endcodeblock %}</p>

<p>{% img center /images/rolls_transcription/roll_keycolumns.jpeg %}</p>

<p>To reconstitute the sheet music the most important is to know when a key
is pressed, not really when it is released. So we will look for the beginning of the holes, i.e. pixels that present a
hole, while the pixel just above them doesn’t.</p>

<p>{% codeblock lang:python %}
# we threshold the picture to separate the pixels
# into ‘hole’ and ‘no-hole’
key_pressed = keys_greyscale &lt; 0.8*keys_greyscale.max()
# We look at the differences between consecutive lines
key_changes =  diff(key_pressed.astype(int), axis=0)</p>

<p>imshow(key_changes)
{% endcodeblock %} </p>

<p>{% img center /images/rolls_transcription/roll_strikes.jpeg %}</p>

<p>This worked quite well: in that picture above red dots indicate key strikes and blue dots indicate key releases. Let us gather all the key strikes in a list.</p>

<p>{% codeblock lang:python %}
Ly, Lx = key_changes.shape
keys_strikes = [(i, j) # (column number, strike time)
                for i in range(Ly)
                for j in range(Lx)
                if key_changes[i, j] == 1]
{% endcodeblock %} </p>

<h2 id="step-2-finding-the-pitch">Step 2: Finding the pitch</h2>

<p>We know that the columns correspond to piano keys. They are sorted left to right from the lowest to the highest note. But which column corresponds to the C4 (the <em>middle C</em>)?</p>

<p>I cheated a little and I looked at the first video (the one where you 
can see the piano keyboard) to see which notes were pressed in the 
first chords. I concluded that C4 is represented by column 34.</p>

<p>From now on I would like the musical notes C4, C#4, D4… to be coded by their respective numbers in the MIDI norm: 60, 61, 62… So I will <em>transpose</em> my list of key strikes by adding 26 to each note.</p>

<p>{% codeblock lang:python %}
transpose = 26
keys_strikes = [(t, key+transpose)
                for t, key in keys_strikes ]
{% endcodeblock %}</p>

<h2 id="step-3-quantization-of-the-notes">Step 3: Quantization of the notes</h2>

<p>We have a list of notes with the time (or frame) at which they are 
played. We will now determine which notes are quarters, which are 
eights, etc. This operation is equivalent to finding the tempo of the 
piece. Let us first have a look at the times at which the the piano keys are striken:</p>

<p>{% codeblock lang:python %}
strike_times = (key_changes == 1).sum(axis=1)
plot(strike_times)
xlabel(‘frame number’); ylabel(‘number of keys hit’)
{% endcodeblock %}</p>

<p>{% img center /images/rolls_transcription/number_keys_hit.jpeg %}</p>

<p>We observe regularly-spaced peaks corresponding to chords (several notes 
striken together). In this kind of music, chords are mainly played on 
the beat. Therefore, computing the main period in the graph above will give us the duration of a beat (or quarter). Let us have a look at the spectrum.</p>

<p>{% codeblock lang:python %}</p>

<p>tt = arange(len(strike_times))
durations = arange(1.1,30,.02) # avoid 1.0
transform = array([fourier_transform(strike_times,d, tt)
                    for d in durations] )
optimal_i = argmax(abs(transform))
quarter_duration = durations[optimal_i]</p>

<p>plot(durations, abs(transform))
xlabel(‘period (in frames)’); ylabel(‘Spectrum value’)
{% endcodeblock %}</p>

<p>{% img center /images/rolls_transcription/notes_spectrum.jpeg %}</p>

<p>The higher peak indicates that a quarter has a duration corresponding to 7.1 frames of the video. Just for info, we can estimate the tempo of the piece with</p>

<p>{% codeblock lang:python %}
tempo = int(video.fps * 60.0/quarter_duration) # we find 252.
{% endcodeblock %}</p>

<p>We will now separate the hands. Let’s keep things simple and say that the left hand takes all the notes below the middle C.</p>

<p>{% codeblock lang:python %}
C4 = 60
left_hand = [(t,key) for (t,key) in keys_strikes if key&lt;C4]
right_hand = [(t,key) for (t,key) in keys_strikes if key&gt;=C4]
{% endcodeblock %}</p>

<p>Then we quantize the notes of each hand with the following algorithm: compute the time duration $d$ between a note and the previous note, and compare $d$ to the duration $Q$ of the quarter:</p>

<ul>
  <li>If $d &lt; Q/4$, consider that the two notes belong to the same
chord.</li>
  <li>Else, if $Q/4 \leq d &lt; 3Q/4$ , consider that the previous note was an
eighth.</li>
  <li>Else, if $ 3Q/4 \leq d &lt; 5Q/4 $, consider that the previous note
was a quarter</li>
  <li>etc.</li>
</ul>

<p>And we treat the notes one after another:</p>

<p>{% codeblock lang:python %}
def quantize(keys_strikes, quarter_duration):</p>

<pre><code># the result is initialized with one 'empty' note.
result = [ {'notes':[], 'duration':None, 't_strike':0} ] 

for time, key in keys_strikes:
    
    # time elapsed since last strike 
    delay = time - result[-1]['t_strike']
    # the next line quantizes that time in eights.
    delay_q = 0.5*int((4.0*delay/quarter_duration+1)/2)
    
    if (delay_q == 0):# put note in previous chord
        if key not in result[-1]['notes']:
            result[-1]['notes'].append(key)
            
    else: # this is a 'new' note/chord
        result[-1]['duration'] = delay_q
        result.append( {'notes': [key], 
                        'duration': None, 
                        't_strike':time} )

result[-1]['duration'] = 4 # give duration to last note

if result[0]['notes'] == []: 
    result.pop(0) # first note will surely be empty
    
return result
</code></pre>

<p>left_hand_quantized = quantize(left_hand, quarter_duration)
right_hand__quantized = quantize(right_hand, quarter_duration)
{% endcodeblock %}</p>

<p>Here is what the final data looks like:</p>

<pre><code>&gt;&gt;&gt; right_hand_q[:4]
#&gt; [{'duration': 1.0, 'notes': [70, 72, 76, 80], 't_strike': 20},
#&gt;  {'duration': 1.0, 'notes': [68, 74, 78, 82], 't_strike': 28},
#&gt;  {'duration': 1.0, 'notes': [66, 76, 80, 84], 't_strike': 35},
#&gt;  {'duration': 1.0, 'notes': [68, 74, 78, 82], 't_strike': 43}]
</code></pre>

<h2 id="step-4-export-to-sheet-music-with-lilypond">Step 4: Export to sheet music with Lilypond</h2>

<p>Our script’s last task is to convert these lists of quantized notes to a music notation language called
<a href="http://www.lilypond.org/index.fr.html">Lilypond</a>, which wan be compiled into high-quality sheet music.
Some packages like music21 can do that, but it is also fairly easy to program your own converter:</p>

<p>{% codeblock lang:python %}
# non-exhaustive lists (but will do for our example)
lilynotes = [‘c’, ‘cis’, ‘d’, ‘ees’, ‘e’, ‘f’,
             ‘fis’, ‘g’, ‘gis’, ‘a’, ‘bes’, ‘b’]
lilyoctaves = [’,,,’,’,,’,’,’,’’,”’”, “’’”, “’’’”]
lilydurations = {0.5:’8’, 1:’4’, 1.5:’4.’, 2:’2’,
                 3: ‘2.’, 4:’1’}</p>

<p>def midi2lily(note):
    “”” converts  60-&gt;c, and 61-&gt;cis, etc. “””
    octave, rank = (note / 12) - 1 , note % 12
    return lilynotes[rank]+lilyoctaves[octave]</p>

<p>def strike2lily(strike):
    “”” converts [60,64],1 -&gt; <c e="">4 """
    notes, duration =strike['notes'], strike['duration']</c></p>

<pre><code>if len(notes) &gt; 1: # chord
    chord = ' '.join(map(midi2lily, sorted(notes)))
    return"&lt; %s &gt;"%chord +lilydurations[duration]
else:
    return midi2lily(notes[0]) +lilydurations[duration]
</code></pre>

<p>def lilyscore(strikes):
    “”” converts a python list of srikes into Lilypond “”” 
    return “\n”.join(map(strike2lily,strikes))</p>

<p>left_hand_lily = lilyscore(left_hand_quantized)
right_hand_lily = lilyscore(right_hand_quantized)
{% endcodeblock %}</p>

<p>Then we just write this lilyfied sheet music in a file and render the sheet music by calling lilypond.</p>

<p>{% codeblock lang:python %}
filename = “limehouse.ly”</p>

<p>with open(filename, ‘w+’) as f:
    f.write(“\score{\new Voice{ \tempo 4=%d”%tempo
            + “\n %s}}”%right_hand_lily)</p>

<h1 id="render-the-sheet-music-by-running-lilypond">render the sheet music by running Lilypond</h1>
<p>import os<br />
os.system(‘lilypond %s’%filename)
{% endcodeblock %}</p>

<p>Here are the first lines of the resulting PDF file, showing the right 
hand’s part.</p>

<p>{% img center /images/rolls_transcription/right_hand_ly.jpg %}</p>

<p>The script has made a pretty good work here, all the notes are there 
with the right pitch and the right duration. If we now transcribe the 
whole piece we will see some mistakes (mostly notes attributed to the 
wrong hand, and more rarely notes with a wrong duration, wrong pitch, etc.), 
which have to be corrected, but still it is pretty cool to have these 
1500 notes crunched in just a few seconds.
The Lylipond editor <a href="http://frescobaldi.org/">Frescobaldi</a> is perfect 
to finish the editing.</p>

<p><a name="final_result"></a></p>

<h2 id="final-result">Final result</h2>

<p>After 3 hours of editing we come to this playable sheet music (<a href="https://github.com/Zulko/-sheet-music--Gerhswin-Limehouse-Nights/blob/master/pdf/Gershwin%20-%20Limehouse%20Nights%20(Piano%20Roll).pdf?raw=true">PDF</a>) and I can tease the keyboard like I’m George Gershwin !</p>

<p>{% youtube V2XCJNZjm4w %}</p>

<p>Ok, it’s just the first bars - I am still unhappy with my rendition of the rest, it’s a pretty demanding piece.</p>

<p>Since the piece is in the public domain I also put my transcription in 
the public domain, and placed its lilypond source <a href="https://github.com/Zulko/-sheet-music--Gerhswin-Limehouse-Nights">here on Github</a> (feel 
free to share/correct/modify it !).</p>

<p>I also wrapped this code into a python package called <a href="http://zulko.github.io/unroll/">Unroll</a> which can 
transcribe from a video of from a midi file (it uses the package 
<em>music21</em> for lilypond conversion, and also provides a convenient LilyPond piano template).</p>

<p>{% codeblock lang:python %}
from unroll import video2scan, rollscan2keystrikes, KeyStrikes
# just transcribe until t=74s, after this it is a repeat.
scan = video2scan(videofile = “limehouse_nights.mp4”,
                  start=2, end=74,
                  focus = lambda im : im[[156],58:478])
keystrikes = rollscan2keystrikes(scan, report=True).transposed(26)
keystrikes.transcribe(‘test2.ly’, quarter_durations =[2,10,0.01])
{% endcodeblock %}</p>

<p>Oh, and that video of me playing was also made with Python (and my library <a href="http://zulko.github.io/moviepy/">MoviePy</a>). Here is <a href="https://gist.github.com/Zulko/10489427">the script</a> that generated it. </p>

<h2 id="a-final-word-on-piano-rolls-transcription">A final word on piano rolls transcription</h2>

<p>I have been transcribing rolls as an occasional hobby for years, and I 
am not the only one: here is 
<a href="http://piyo.ciao.jp/sm/main.html">another</a> transcriber, and 
<a href="http://ragtime-france.fr/Ragtime/indexUS.htm">another</a> and yet <a href="http://www.moltoallegro.com">another</a>. Even <em>Limehouse Nights</em> has apparently been <a href="http://www.youtube.com/watch?v=t9o5a7G4l20">recorded</a> in 1992 but the pianist didn’t publish his transcription.</p>

<p>Most of us transcribe from MIDI files which are made from piano rolls 
scans (starting from MIDI files is equivalent to starting directly to 
Step 3, quantization and hands separation). Thousands of MIDI files 
from rolls scans are available on the internet (like 
<a href="http://www.iammp.org">here</a> or <a href="http://www.pianola.co.nz">here</a>) but 
not all mechanical piano owners have the appropriate scanner, so 
there must be thousands of other rolls in private collections which 
have never been scanned and pushed on the Internet.</p>

<p>I wrote this post to show that just filming piano rolls in action is enough for transcriptions purposes.</p>
]]></content>
  </entry>
  
</feed>
